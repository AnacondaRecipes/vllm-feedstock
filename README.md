# vllm-feedstock
A conda recipe for vllm
vLLM is a fast and easy-to-use library for LLM inference and serving and a high-throughput and memory-efficient inference and serving engine for LLMs.

## Home:
https://github.com/vllm-project/vllm/tree/v0.3.3

## Upstream license:
Apache-2.0

# Feedstock license:
BSD-3

## Docs
https://docs.vllm.ai

## Dev:
https://github.com/vllm-project/vllm/tree/v0.3.3