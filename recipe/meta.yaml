{% set name = "vllm" %}
{% set version = "0.6.4.post1" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://files.pythonhosted.org/packages/06/ab/2d562d5d4475f1a071b831564971752c085830793a6d9033383926d5eb9c/vllm-0.6.4.post1-cp38-abi3-manylinux1_x86_64.whl
  sha256: dc151793688376904ca54129a4aa0b83aed9d7ad8e458666775f62b37ecbddcc
    #  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz
    #  sha256: 24b70159bbcfd441bfa9d3e226ba8f5db74837c5325fea4a2104cf46c5d8246e

build:
  # script: |
    #export CUDA_HOME=$SRC_DIR
    # {{ PYTHON }} -m pip install . --no-deps --no-build-isolation -vv
  number: 0

requirements:
  host:
    - python
    - ninja
    - packaging
    - setuptools
    - pytorch >=2.3.1
    - wheel
    - pip
    - cudatoolkit
    - transformers
    - psutil
    - msgspec >=0.19
    - pydantic
      #    - mistral_common
  run:
    - python
    - pytorch >=2.1.2
    - cudatoolkit
    - transformers
    - psutil
    - msgspec >=0.19
    - pydantic
      #   - mistral_common
test:
  imports:
    - vllm
  commands:
    - pip check
  requires:
    - pip

about:
  home: https://github.com/vllm-project/vllm/tree/v0.3.3
  summary: vLLM is a fast and easy-to-use library for LLM inference and serving.
  description: A high-throughput and memory-efficient inference and serving engine for LLMs.
  license: Apache-2.0
    #  license_file:
    #- LICENSE
    #- csrc/punica/LICENSE
    #- csrc/quantization/marlin/LICENSE
  doc_url: https://docs.vllm.ai
  dev_url: https://github.com/vllm-project/vllm/tree/v0.3.3

extra:
  anaconda-services:
    ref: https://anaconda.zendesk.com/agent/tickets/49522
    reason: Client request
  recipe-maintainers:
    - ianaobi
  recipe-maintainers:
    - boldorider4
